{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabulary\n",
    "with open('helpers/vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_pos.txt', 'r') as f:\n",
    "    pos_tweets = f.read().splitlines()\n",
    "with open('data/train_neg.txt', 'r') as f:\n",
    "    neg_tweets = f.read().splitlines()\n",
    "embeddings = np.load('helpers/embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = np.array(pos_tweets + neg_tweets)\n",
    "y = np.array(([1] * len(pos_tweets)) + ([0] * len(neg_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local validation set\n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    #np.random.seed(seed)\n",
    "    # generate random indices\n",
    "    num_row = len(y)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    index_split = int(np.floor(ratio * num_row))\n",
    "    index_tr = indices[: index_split]\n",
    "    index_te = indices[index_split:]\n",
    "    # create split\n",
    "    x_tr = x[index_tr]\n",
    "    x_te = x[index_te]\n",
    "    y_tr = y[index_tr]\n",
    "    y_te = y[index_te]\n",
    "    return x_tr, x_te, y_tr, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_te, y_tr, y_te = split_data(train_tweets, y, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; &lt;user&gt; who asked miss mcilroy here ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11 \" latex \" just married ! \" balloons ( pack of 12 latex \" just married ! \" balloons . need an inexpensive , fun decora ... &lt;url&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oh , jaimie . . this is maya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; noomm ! xd aha not much ; so bored : l aha want to watch a film but don't know which one ahha &lt;3 xxx</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i can't explain how i feel about &lt;user&gt; losing points again at home #comeonandwinthefacup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                               tweets  \\\n",
       "0  <user> <user> <user> who asked miss mcilroy here ?                                                                                   \n",
       "1  11 \" latex \" just married ! \" balloons ( pack of 12 latex \" just married ! \" balloons . need an inexpensive , fun decora ... <url>   \n",
       "2  oh , jaimie . . this is maya                                                                                                         \n",
       "3  <user> noomm ! xd aha not much ; so bored : l aha want to watch a film but don't know which one ahha <3 xxx                          \n",
       "4  i can't explain how i feel about <user> losing points again at home #comeonandwinthefacup                                            \n",
       "\n",
       "   class  \n",
       "0  1      \n",
       "1  0      \n",
       "2  1      \n",
       "3  0      \n",
       "4  0      "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_pd = pd.DataFrame({'tweets': x_tr, 'class': y_tr})\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "x_tr_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>who asked miss mcilroy here</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>latex   just married     balloons   pack of    latex   just married     balloons   need an inexpensive   fun decora</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oh   jaimie     this is maya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>noomm   xd aha not much   so bored   l aha want to watch a film but don t know which one ahha    xxx</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i can t explain how i feel about   losing points again at home #comeonandwinthefacup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           tweets  \\\n",
       "0        who asked miss mcilroy here                                                                                                \n",
       "1       latex   just married     balloons   pack of    latex   just married     balloons   need an inexpensive   fun decora         \n",
       "2  oh   jaimie     this is maya                                                                                                     \n",
       "3    noomm   xd aha not much   so bored   l aha want to watch a film but don t know which one ahha    xxx                           \n",
       "4  i can t explain how i feel about   losing points again at home #comeonandwinthefacup                                             \n",
       "\n",
       "   class  \n",
       "0  1      \n",
       "1  0      \n",
       "2  1      \n",
       "3  0      \n",
       "4  0      "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing usertags, urls, numbers and special characters\n",
    "to_remove = ['<user>', '<url>', '[^a-zA-Z#]']\n",
    "x_tr_pd.tweets = x_tr_pd.tweets.replace(to_remove, ' ', regex=True)\n",
    "x_tr_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all words smaller than 3 characters\n",
    "x_tr_pd.tweets = x_tr_pd.tweets.replace([r'\\b\\w{,3}\\b'], '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "x_tr_pd.tweets = x_tr_pd.tweets.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asked miss mcilroy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>latex married balloons pack latex married balloons need inexpensive decora</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jaimie maya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>noomm much bored want watch film know ahha</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>explain feel losing points home #comeonandwinthefacup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hehehe like janoskians georgie</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>haha dont knock tried first time everything</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>psychoholic friday nightmare darkness satan #mymortuary</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>introduction persian revised edition hardcover comprehensive grammar modern classical collo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>anxiety issues presentations</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>moments</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>still sick suckin little sisters play #greatestbigsis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nursing assisting challenge site license question program great assess rein</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>wonder spotted tiny bails</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dont worry periodic check clnica cemtro</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lost followers cause tweet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>wait baby thursday #tooexcited</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>physician endorsed belle khaki physician endorsed belle cotton canvas inch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bulletproof audio limited seven inch shaped picture disc vinyl pressing single lifted</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>wanted take time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         tweets  \\\n",
       "0   asked miss mcilroy                                                                            \n",
       "1   latex married balloons pack latex married balloons need inexpensive decora                    \n",
       "2   jaimie maya                                                                                   \n",
       "3   noomm much bored want watch film know ahha                                                    \n",
       "4   explain feel losing points home #comeonandwinthefacup                                         \n",
       "5   hehehe like janoskians georgie                                                                \n",
       "6   haha dont knock tried first time everything                                                   \n",
       "7   psychoholic friday nightmare darkness satan #mymortuary                                       \n",
       "8   introduction persian revised edition hardcover comprehensive grammar modern classical collo   \n",
       "9   anxiety issues presentations                                                                  \n",
       "10  moments                                                                                       \n",
       "11  still sick suckin little sisters play #greatestbigsis                                         \n",
       "12  nursing assisting challenge site license question program great assess rein                   \n",
       "13  wonder spotted tiny bails                                                                     \n",
       "14  dont worry periodic check clnica cemtro                                                       \n",
       "15  lost followers cause tweet                                                                    \n",
       "16  wait baby thursday #tooexcited                                                                \n",
       "17  physician endorsed belle khaki physician endorsed belle cotton canvas inch                    \n",
       "18  bulletproof audio limited seven inch shaped picture disc vinyl pressing single lifted         \n",
       "19  wanted take time                                                                              \n",
       "\n",
       "    class  \n",
       "0   1      \n",
       "1   0      \n",
       "2   1      \n",
       "3   0      \n",
       "4   0      \n",
       "5   1      \n",
       "6   1      \n",
       "7   1      \n",
       "8   0      \n",
       "9   0      \n",
       "10  1      \n",
       "11  0      \n",
       "12  0      \n",
       "13  1      \n",
       "14  1      \n",
       "15  0      \n",
       "16  1      \n",
       "17  0      \n",
       "18  0      \n",
       "19  0      "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_pd.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love      8933\n",
      "like      7413\n",
      "frame     6369\n",
      "good      6226\n",
      "know      6185\n",
      "follow    5826\n",
      "please    4962\n",
      "want      4673\n",
      "back      4576\n",
      "today     4297\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(x_tr_pd.tweets).split()).value_counts()[:10]\n",
    "print(freq)\n",
    "popular_words = list(freq.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove popular words\n",
    "x_tr_pd.tweets = x_tr_pd.tweets.apply(lambda x: \" \".join(x for x in x.split() if x not in popular_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asked miss mcilroy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>latex married balloons pack latex married balloons need inexpensive decora</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jaimie maya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>noomm much bored watch film ahha</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>explain feel losing points home #comeonandwinthefacup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hehehe janoskians georgie</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>haha dont knock tried first time everything</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>psychoholic friday nightmare darkness satan #mymortuary</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>introduction persian revised edition hardcover comprehensive grammar modern classical collo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>anxiety issues presentations</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>moments</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>still sick suckin little sisters play #greatestbigsis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nursing assisting challenge site license question program great assess rein</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>wonder spotted tiny bails</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dont worry periodic check clnica cemtro</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lost followers cause tweet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>wait baby thursday #tooexcited</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>physician endorsed belle khaki physician endorsed belle cotton canvas inch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bulletproof audio limited seven inch shaped picture disc vinyl pressing single lifted</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>wanted take time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         tweets  \\\n",
       "0   asked miss mcilroy                                                                            \n",
       "1   latex married balloons pack latex married balloons need inexpensive decora                    \n",
       "2   jaimie maya                                                                                   \n",
       "3   noomm much bored watch film ahha                                                              \n",
       "4   explain feel losing points home #comeonandwinthefacup                                         \n",
       "5   hehehe janoskians georgie                                                                     \n",
       "6   haha dont knock tried first time everything                                                   \n",
       "7   psychoholic friday nightmare darkness satan #mymortuary                                       \n",
       "8   introduction persian revised edition hardcover comprehensive grammar modern classical collo   \n",
       "9   anxiety issues presentations                                                                  \n",
       "10  moments                                                                                       \n",
       "11  still sick suckin little sisters play #greatestbigsis                                         \n",
       "12  nursing assisting challenge site license question program great assess rein                   \n",
       "13  wonder spotted tiny bails                                                                     \n",
       "14  dont worry periodic check clnica cemtro                                                       \n",
       "15  lost followers cause tweet                                                                    \n",
       "16  wait baby thursday #tooexcited                                                                \n",
       "17  physician endorsed belle khaki physician endorsed belle cotton canvas inch                    \n",
       "18  bulletproof audio limited seven inch shaped picture disc vinyl pressing single lifted         \n",
       "19  wanted take time                                                                              \n",
       "\n",
       "    class  \n",
       "0   1      \n",
       "1   0      \n",
       "2   1      \n",
       "3   0      \n",
       "4   0      \n",
       "5   1      \n",
       "6   1      \n",
       "7   1      \n",
       "8   0      \n",
       "9   0      \n",
       "10  1      \n",
       "11  0      \n",
       "12  0      \n",
       "13  1      \n",
       "14  1      \n",
       "15  0      \n",
       "16  1      \n",
       "17  0      \n",
       "18  0      \n",
       "19  0      "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_pd.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate number of words that appear just once\n",
    "freq = pd.Series(' '.join(x_tr_pd.tweets).split()).value_counts()\n",
    "dict_words = freq.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_pd.tweets = x_tr_pd.tweets.apply(lambda x: \" \".join(x for x in x.split() if dict_words[x] > 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asked miss</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>latex married balloons pack latex married balloons need inexpensive decora</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jaimie maya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>much bored watch film ahha</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>explain feel losing points home</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hehehe janoskians georgie</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>haha dont knock tried first time everything</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>friday nightmare darkness satan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>introduction persian revised edition hardcover comprehensive grammar modern classical</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>anxiety issues presentations</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>moments</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>still sick suckin little sisters play</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nursing assisting challenge site license question program great rein</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>wonder spotted tiny</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dont worry periodic check</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lost followers cause tweet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>wait baby thursday #tooexcited</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>physician endorsed belle khaki physician endorsed belle cotton canvas inch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>audio limited seven inch shaped picture disc vinyl pressing single lifted</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>wanted take time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                   tweets  \\\n",
       "0   asked miss                                                                              \n",
       "1   latex married balloons pack latex married balloons need inexpensive decora              \n",
       "2   jaimie maya                                                                             \n",
       "3   much bored watch film ahha                                                              \n",
       "4   explain feel losing points home                                                         \n",
       "5   hehehe janoskians georgie                                                               \n",
       "6   haha dont knock tried first time everything                                             \n",
       "7   friday nightmare darkness satan                                                         \n",
       "8   introduction persian revised edition hardcover comprehensive grammar modern classical   \n",
       "9   anxiety issues presentations                                                            \n",
       "10  moments                                                                                 \n",
       "11  still sick suckin little sisters play                                                   \n",
       "12  nursing assisting challenge site license question program great rein                    \n",
       "13  wonder spotted tiny                                                                     \n",
       "14  dont worry periodic check                                                               \n",
       "15  lost followers cause tweet                                                              \n",
       "16  wait baby thursday #tooexcited                                                          \n",
       "17  physician endorsed belle khaki physician endorsed belle cotton canvas inch              \n",
       "18  audio limited seven inch shaped picture disc vinyl pressing single lifted               \n",
       "19  wanted take time                                                                        \n",
       "\n",
       "    class  \n",
       "0   1      \n",
       "1   0      \n",
       "2   1      \n",
       "3   0      \n",
       "4   0      \n",
       "5   1      \n",
       "6   1      \n",
       "7   1      \n",
       "8   0      \n",
       "9   0      \n",
       "10  1      \n",
       "11  0      \n",
       "12  0      \n",
       "13  1      \n",
       "14  1      \n",
       "15  0      \n",
       "16  1      \n",
       "17  0      \n",
       "18  0      \n",
       "19  0      "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_pd.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asked miss</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>latex married balloon pack latex married balloon need inexpensive decora</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jaimie maya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>much bored watch film ahha</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>explain feel losing point home</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hehehe janoskians georgie</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>haha dont knock tried first time everything</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>friday nightmare darkness satan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>introduction persian revised edition hardcover comprehensive grammar modern classical</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>anxiety issue presentation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>moment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>still sick suckin little sister play</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nursing assisting challenge site license question program great rein</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>wonder spotted tiny</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dont worry periodic check</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lost follower cause tweet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>wait baby thursday #tooexcited</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>physician endorsed belle khaki physician endorsed belle cotton canvas inch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>audio limited seven inch shaped picture disc vinyl pressing single lifted</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>wanted take time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                   tweets  \\\n",
       "0   asked miss                                                                              \n",
       "1   latex married balloon pack latex married balloon need inexpensive decora                \n",
       "2   jaimie maya                                                                             \n",
       "3   much bored watch film ahha                                                              \n",
       "4   explain feel losing point home                                                          \n",
       "5   hehehe janoskians georgie                                                               \n",
       "6   haha dont knock tried first time everything                                             \n",
       "7   friday nightmare darkness satan                                                         \n",
       "8   introduction persian revised edition hardcover comprehensive grammar modern classical   \n",
       "9   anxiety issue presentation                                                              \n",
       "10  moment                                                                                  \n",
       "11  still sick suckin little sister play                                                    \n",
       "12  nursing assisting challenge site license question program great rein                    \n",
       "13  wonder spotted tiny                                                                     \n",
       "14  dont worry periodic check                                                               \n",
       "15  lost follower cause tweet                                                               \n",
       "16  wait baby thursday #tooexcited                                                          \n",
       "17  physician endorsed belle khaki physician endorsed belle cotton canvas inch              \n",
       "18  audio limited seven inch shaped picture disc vinyl pressing single lifted               \n",
       "19  wanted take time                                                                        \n",
       "\n",
       "    class  \n",
       "0   1      \n",
       "1   0      \n",
       "2   1      \n",
       "3   0      \n",
       "4   0      \n",
       "5   1      \n",
       "6   1      \n",
       "7   1      \n",
       "8   0      \n",
       "9   0      \n",
       "10  1      \n",
       "11  0      \n",
       "12  0      \n",
       "13  1      \n",
       "14  1      \n",
       "15  0      \n",
       "16  1      \n",
       "17  0      \n",
       "18  0      \n",
       "19  0      "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "x_tr_pd.tweets = x_tr_pd.tweets.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "x_tr_pd.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([np.zeros(len(embeddings[0]))])\n",
    "Y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 10000\n",
      "iteration 20000\n",
      "iteration 30000\n",
      "iteration 40000\n",
      "iteration 50000\n",
      "iteration 60000\n",
      "iteration 70000\n",
      "iteration 80000\n",
      "iteration 90000\n",
      "iteration 100000\n",
      "iteration 110000\n",
      "iteration 120000\n",
      "iteration 130000\n",
      "iteration 140000\n",
      "iteration 150000\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for tweet in x_tr_pd.tweets:\n",
    "    if(i % 10000 == 0): print(\"iteration %d\" % i)\n",
    "    words_vocab = [vocab.get(word, -1) for word in tweet.split()] # replace with -1 if not in vocab\n",
    "    words_vocab = [w_ind for w_ind in words_vocab if w_ind > -1] # filter away inexisting words\n",
    "    average_vector = np.array([np.zeros(20)])\n",
    "    if len(words_vocab) > 0: # only apply average for tweets which have words in the vocab\n",
    "        words_emb = [embeddings[w_v] for w_v in words_vocab]\n",
    "        average_vector = (np.sum(words_emb, axis=0)/len(words_emb)).reshape(1,20)\n",
    "    X = np.append(X, average_vector, axis=0)\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160000, 20)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.ones((X.shape[0],X.shape[1]+1))\n",
    "X_new[:,:-1] = X\n",
    "Y = x_tr_pd['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ada/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.1)\n",
    "lr.fit(X_new, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ada/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2 = LogisticRegression(C=1)\n",
    "lr2.fit(X_new, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#biggestregret nothing never regret live thankful experience would past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high speed hdmi cable foot meter product quality electronics accessory offered great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mcfly last week came obsessed worry else like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robyn sleeping broken heart radio live lounge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>quick become</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 tweets\n",
       "0  #biggestregret nothing never regret live thankful experience would past             \n",
       "1  high speed hdmi cable foot meter product quality electronics accessory offered great\n",
       "2  mcfly last week came obsessed worry else like                                       \n",
       "3  robyn sleeping broken heart radio live lounge                                       \n",
       "4  quick become                                                                        "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = pd.DataFrame(x_te, columns=['tweets'])\n",
    "m.tweets = m.tweets.replace(to_remove, ' ', regex=True)\n",
    "m.tweets = m.tweets.replace([r'\\b\\w{,3}\\b'], '', regex=True)\n",
    "m.tweets = m.tweets.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "m.tweets = m.tweets.apply(lambda x: \" \".join(x for x in x.split() if x not in popular_words))\n",
    "freq = pd.Series(' '.join(m.tweets).split()).value_counts()\n",
    "dict_words = freq.to_dict()\n",
    "m.tweets = m.tweets.apply(lambda x: \" \".join(x for x in x.split() if dict_words[x] > 1))\n",
    "m.tweets = m.tweets.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([np.zeros(len(embeddings[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 10000\n",
      "iteration 20000\n",
      "iteration 30000\n"
     ]
    }
   ],
   "source": [
    "# remake test tweets into word embeddings\n",
    "i = 0\n",
    "for tweet in m.tweets:\n",
    "    if(i % 10000 == 0): print(\"iteration %d\" % i)\n",
    "    words_vocab = [vocab.get(word, -1) for word in tweet.split()] # replace with -1 if not in vocab\n",
    "    words_vocab = [w_ind for w_ind in words_vocab if w_ind > -1] # filter away inexisting words\n",
    "    average_vector = np.array([np.zeros(20)])\n",
    "    if len(words_vocab) > 0:\n",
    "        words_emb = [embeddings[w_v] for w_v in words_vocab]\n",
    "        average_vector = (np.sum(words_emb, axis=0)/len(words_emb)).reshape(1,20)\n",
    "    X_test = np.append(X_test, average_vector, axis=0) \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new = np.ones((X_test.shape[0],X_test.shape[1]+1))\n",
    "X_test_new[:,:-1] = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = lr.predict(X_test_new)\n",
    "y_test2 = lr2.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y1, y2):\n",
    "    indices = np.arange(len(y1))[y1==y2]\n",
    "    return len(indices)/len(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5592\n",
      "0.5592\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(y_test, y_te))\n",
    "print(accuracy(y_test2, y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try on actual test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test_data.txt', 'r') as f:\n",
    "    test_tweets = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scooter sport portable stay longer water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>well work week come cheer battery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cant stay away thats baby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>perfectly fine anymore lmao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>whenever fall asleep watching always wake headache</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweets\n",
       "0  scooter sport portable stay longer water          \n",
       "1  well work week come cheer battery                 \n",
       "2  cant stay away thats baby                         \n",
       "3  perfectly fine anymore lmao                       \n",
       "4  whenever fall asleep watching always wake headache"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = pd.DataFrame(test_tweets, columns=['tweets'])\n",
    "m.tweets = m.tweets.replace(to_remove, ' ', regex=True)\n",
    "m.tweets = m.tweets.replace([r'\\b\\w{,3}\\b'], '', regex=True)\n",
    "m.tweets = m.tweets.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "m.tweets = m.tweets.apply(lambda x: \" \".join(x for x in x.split() if x not in popular_words))\n",
    "freq = pd.Series(' '.join(m.tweets).split()).value_counts()\n",
    "dict_words = freq.to_dict()\n",
    "m.tweets = m.tweets.apply(lambda x: \" \".join(x for x in x.split() if dict_words[x] > 1))\n",
    "m.tweets = m.tweets.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.array([np.zeros(len(embeddings[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake test tweets into word embeddings\n",
    "for tweet in m.tweets:\n",
    "    words_vocab = [vocab.get(word, -1) for word in tweet.split()] # replace with -1 if not in vocab\n",
    "    words_vocab = [w_ind for w_ind in words_vocab if w_ind > -1] # filter away inexisting words\n",
    "    average_vector = np.array([np.zeros(20)])\n",
    "    if len(words_vocab) > 0:\n",
    "        words_emb = [embeddings[w_v] for w_v in words_vocab]\n",
    "        average_vector = (np.sum(words_emb, axis=0)/len(words_emb)).reshape(1,20)\n",
    "    test_X = np.append(test_X, average_vector, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_X[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = np.ones((test_X.shape[0], test_X.shape[1]+1))\n",
    "X_final[:,:-1] = test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "ja = lr2.predict(X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_result.csv', 'w') as csvfile:\n",
    "    tempwriter = csv.writer(csvfile)\n",
    "    tempwriter.writerow([\"Id\",\"Prediction\"])\n",
    "    count = 1\n",
    "    for row in ja:\n",
    "        if row == 0:\n",
    "            row = -1\n",
    "        tempwriter.writerow([count,str(row)])\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
