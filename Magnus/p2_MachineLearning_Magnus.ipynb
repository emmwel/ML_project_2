{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification of tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Magnu\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, Doc2Vec, FastText\n",
    "import gensim.parsing.preprocessing as prep\n",
    "import csv\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets, svm\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#import seaborn\n",
    "#seaborn.use()\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all functions ready\n"
     ]
    }
   ],
   "source": [
    "def open_file(fileName):\n",
    "    with open(str(fileName), \"r\", encoding=\"utf8\") as sample:\n",
    "        s = sample.readlines()\n",
    "    return s\n",
    "\n",
    "\n",
    "def create_X(list_of_tweets, w2v, features):\n",
    "\n",
    "    # this function needs some love!\n",
    "\n",
    "    X = np.zeros((len(list_of_tweets), features))\n",
    "\n",
    "    for indeks, tweet in enumerate(list_of_tweets):\n",
    "        for word in tweet:\n",
    "            try:\n",
    "                X[indeks, :] = X[indeks, :] + model_tot.wv[str(word)]\n",
    "            except:\n",
    "                pass\n",
    "        N = len(tweet)\n",
    "        if N > 0:\n",
    "            X[indeks] = X[indeks] / N\n",
    "    return X\n",
    "\n",
    "\n",
    "def createWordEmbedding(list_of_tweets, features, epoc, sg=0):\n",
    "    print(sg)\n",
    "    model = Word2Vec(\n",
    "        list_of_tweets, size=features, window=5, min_count=1, workers=4, sg=sg)\n",
    "    print('word embedding created, start training:')\n",
    "    model.train(\n",
    "        list_of_tweets, total_examples=len(list_of_tweets), epochs=epoc)\n",
    "    return model\n",
    "\n",
    "\n",
    "def createSentEmbedding(list_of_tweets, features, epoc):\n",
    "    model = Doc2Vec(list_of_tweets, size=features, min_count=1)\n",
    "    model.train(\n",
    "        list_of_tweets, total_examples=len(list_of_tweets), epochs=epoc)\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_csv(fileName, test_y):\n",
    "    ids = np.arange(len(test_y))\n",
    "    with open(fileName, 'w') as csvfile:\n",
    "        tempwriter = csv.writer(csvfile)\n",
    "        tempwriter.writerow([\"Id\", \"Prediction\"])\n",
    "        count = 0\n",
    "        for row in test_y:\n",
    "            if row == 0:\n",
    "                row = -1\n",
    "            tempwriter.writerow([(ids[count]) + 1, str(row)])\n",
    "            count = count + 1\n",
    "\n",
    "\n",
    "def train(method, x, y, x_test):\n",
    "    met = method.fit(x, y)\n",
    "    test_y = met.predict(x_test)\n",
    "    return test_y\n",
    "\n",
    "\n",
    "def shuffle_tweets(listTweets, y):\n",
    "    c = list(zip(listTweets, y))\n",
    "    random.shuffle(c)\n",
    "    listTweets, y = zip(*c)\n",
    "    return listTweets, y\n",
    "\n",
    "\n",
    "def processTrainingData(list_of_tweets):\n",
    "    list_of_tweets = [prep.strip_short(line) for line in list_of_tweets]\n",
    "    list_of_tweets = prep.preprocess_documents(list_of_tweets)\n",
    "    return list_of_tweets\n",
    "\n",
    "\n",
    "def easyProcess(list_of_tweets):\n",
    "    list_of_tweets = [prep.strip_short(line) for line in list_of_tweets]\n",
    "    return list_of_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "loaded and processed short training set\n"
     ]
    }
   ],
   "source": [
    "positive_tweets = open_file(\"train_pos.txt\")\n",
    "negative_tweets = open_file(\"train_neg.txt\")\n",
    "\n",
    "positive_tweets = processTrainingData(positive_tweets)\n",
    "negative_tweets = processTrainingData(negative_tweets)\n",
    "y = [1] * len(positive_tweets) + [0] * len(negative_tweets)\n",
    "\n",
    "all_tweets = positive_tweets + negative_tweets\n",
    "all_tweets, y = shuffle_tweets(all_tweets, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create word embedding based on full set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Creates word embeddings for full training set:\\n\\npositive_tweets_full = open_file(\"train_pos_full.txt\")\\nnegative_tweets_full = open_file(\"train_neg_full.txt\")\\nprint(\\'all loaded\\')\\npositive_tweets_full = processTrainingData(positive_tweets_full)\\nnegative_tweets_full = processTrainingData(negative_tweets_full)\\ny_full = [1]*len(positive_tweets_full)+[0]*len(negative_tweets_full)\\nprint(\\'finished with processing\\')\\nall_tweets_full = positive_tweets_full+negative_tweets_full #list of tweets\\nall_tweets_full, y_full = shuffle_tweets(all_tweets_full, y_full)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Creates word embeddings for full training set:\n",
    "\n",
    "positive_tweets_full = open_file(\"train_pos_full.txt\")\n",
    "negative_tweets_full = open_file(\"train_neg_full.txt\")\n",
    "print('all loaded')\n",
    "positive_tweets_full = processTrainingData(positive_tweets_full)\n",
    "negative_tweets_full = processTrainingData(negative_tweets_full)\n",
    "y_full = [1]*len(positive_tweets_full)+[0]*len(negative_tweets_full)\n",
    "print('finished with processing')\n",
    "all_tweets_full = positive_tweets_full+negative_tweets_full #list of tweets\n",
    "all_tweets_full, y_full = shuffle_tweets(all_tweets_full, y_full)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfeatures = 350 # we should find optimal value for this:\\nepoch = 10 # we should fin optimal value for this:\\nmodel_tot = createWordEmbedding(all_tweets, features, epoch) #word embedding\\nprint('word embedding created')\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = 200\n",
    "epoch = 10\n",
    "#print('let the word embedding commence')\n",
    "#model_tot_sg1 = createWordEmbedding(all_tweets_full, features, epoch, sg=1) #word embedding\n",
    "#model_tot_sg0 = createWordEmbedding(all_tweets_full, features, epoch) #word embedding\n",
    "#print('word embedding created')\n",
    "'''\n",
    "features = 350 # we should find optimal value for this:\n",
    "epoch = 10 # we should fin optimal value for this:\n",
    "model_tot = createWordEmbedding(all_tweets, features, epoch) #word embedding\n",
    "print('word embedding created')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_tot_sg1.save('model_tot_sg1_200.word2vec')\n",
    "#model_tot_sg0.save('model_tot_200.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tot = Word2Vec.load(\"model_tot_200.word2vec\")\n",
    "model_tot_sg1 = Word2Vec.load('model_tot_sg1_200.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed feature matrix for tweets\n"
     ]
    }
   ],
   "source": [
    "X_old = create_X(all_tweets, model_tot, features)\n",
    "X_old = preprocessing.scale(X_old)\n",
    "print('processed feature matrix for tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b = np.ones((X.shape[0], X.shape[1]+1)); b[:,:-1] = X\n",
    "X = np.ones((X_old.shape[0], X_old.shape[1] + 1))\n",
    "X[:, 1:] = X_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded test file\n",
      "['1,sea doo pro sea scooter ( sports with the portable sea-doo seascootersave air , stay longer in the water and ... <url>\\n', \"2,<user> shucks well i work all week so now i can't come cheer you on ! oh and put those batteries in your calculator ! ! !\\n\"]\n"
     ]
    }
   ],
   "source": [
    "testd_tweets = open_file(\"test_data.txt\")\n",
    "testd = processTrainingData(testd_tweets)\n",
    "print('loaded test file')\n",
    "print(testd_tweets[:2])\n",
    "print(testd[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    }
   ],
   "source": [
    "X_test_old = create_X(testd, model_tot, features)\n",
    "X_test_old = preprocessing.scale(X_test_old)\n",
    "X_test = np.ones((X_test_old.shape[0], X_test_old.shape[1] + 1))\n",
    "X_test[:, 1:] = X_test_old\n",
    "print('start training')\n",
    "\n",
    "# Build logistic regression classifiers to identify the polarity of words\n",
    "#test_y_lr = train(LogisticRegression(solver='lbfgs', max_iter = 1000), X, y, X_test)\n",
    "\n",
    "# Build naive bayes classifiers to identify the polarity of words\n",
    "#test_y_nb = train(nb.GaussianNB(), X, y, X_test) # this one isn't working\n",
    "\n",
    "#print('created classification for submission')\n",
    "\n",
    "#save_csv('test_resultLR.csv', test_y_lr)\n",
    "#print('saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing some cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.768815\n",
      "done with cross validation\n"
     ]
    }
   ],
   "source": [
    "cv_results_lr = cross_validate(\n",
    "    LogisticRegression(solver='lbfgs'), X, y, return_train_score=False, cv=5)\n",
    "print(np.mean(np.asarray(cv_results_lr['test_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "shuffled\n"
     ]
    }
   ],
   "source": [
    "positive = open_file(\"train_pos.txt\")\n",
    "negative = open_file(\"train_neg.txt\")\n",
    "\n",
    "list_all_tweets = positive + negative\n",
    "y_full = [1] * len(positive) + [0] * len(negative)\n",
    "print(len(y))\n",
    "list_all_tweets, y_full = shuffle_tweets(list_all_tweets, y_full)\n",
    "list_all_tweets = easyProcess(list_all_tweets)\n",
    "print('shuffled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(\n",
    "    list_all_tweets, y_full, test_size=.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000 180000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_tot_k = KeyedVectors.load('model_tot_200.word2vec')\n",
    "model_tot_k2 = KeyedVectors.load('model_tot_sg1_200.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375351"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_tot_k.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 375351 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "for w in model_tot_k.wv.vocab.keys():\n",
    "    embeddings_index[w] = model_tot_k.wv[\n",
    "        w]  #np.append(model_tot_k.wv[w],model_tot_k2.wv[w])\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hubby #toosexy bday ... had lots fun #umph <url>\n",
      "justice vol hardcover eisner award-winning painter alex ross has stunned fans time and again with his ... <url>\n",
      "hmm that's cute watch being passed around des moines and posted facebook\n",
      "dont forget please ...\n",
      "<user> because shes dog and very playfull lol\n"
     ]
    }
   ],
   "source": [
    "for x in x_train[:5]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "for x in x_train:\n",
    "    length.append(len(x.split()))\n",
    "max(length)\n",
    "length = max(length) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (180000, 35)\n"
     ]
    }
   ],
   "source": [
    "x_train_seq = pad_sequences(sequences, maxlen=length)\n",
    "print('Shape of data tensor:', x_train_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_val = tokenizer.texts_to_sequences(x_validation)\n",
    "x_val_seq = pad_sequences(sequences_val, maxlen=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 100000\n",
    "embedding_matrix = np.zeros((num_words, features))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000 180000 180000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), len(y_train), len(x_train_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 35, 200)           20000000  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 34, 100)           40100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "=================================================================\n",
      "Total params: 20,040,100\n",
      "Trainable params: 20,040,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hFeatures = features * 0.5\n",
    "structure_test = Sequential()\n",
    "e = Embedding(100000, features, input_length=length)\n",
    "structure_test.add(e)\n",
    "structure_test.add(\n",
    "    Conv1D(\n",
    "        filters=100,\n",
    "        kernel_size=2,\n",
    "        padding='valid',\n",
    "        activation='relu',\n",
    "        strides=1))\n",
    "structure_test.add(GlobalMaxPooling1D())\n",
    "structure_test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-e55d5c87bb31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel_cnn_03\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel_cnn_03\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel_cnn_03\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_cnn_03 = Sequential()\n",
    "e = Embedding(\n",
    "    100000,\n",
    "    features,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=length,\n",
    "    trainable=True)\n",
    "model_cnn_03.add(e)\n",
    "model_cnn_03.add(\n",
    "    Conv1D(\n",
    "        filters=32,\n",
    "        kernel_size=2,\n",
    "        padding='valid',\n",
    "        activation='relu',\n",
    "        strides=1))\n",
    "model_cnn_03.add(GlobalMaxPooling1D())\n",
    "model_cnn_03.add(Dense(128, activation='relu'))\n",
    "model_cnn_03.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_03.compile(\n",
    "    loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn_03.fit(\n",
    "    x_train_seq,\n",
    "    y_train,\n",
    "    validation_data=(x_val_seq, y_validation),\n",
    "    epochs=2,\n",
    "    batch_size=32,\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_input = Input(shape=(length,), dtype='int32')\n",
    "\n",
    "tweet_encoder = Embedding(100000, features, weights=[embedding_matrix], input_length=length, trainable=True)(tweet_input)\n",
    "bigram_branch = Conv1D(filters=features*0.5, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=features*0.5, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=features*0.5, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = Dense(1)(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"CNN_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.fit(\n",
    "    x_train_seq,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=2,\n",
    "    validation_data=(x_val_seq, y_validation),\n",
    "    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loaded_CNN_model = load_model('CNN_best_weights.02-0.8333.hdf5')\n",
    "loaded_CNN_model.evaluate(x=x_val_seq, y=y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testd_tweets = easyProcess(testd_tweets)\n",
    "\n",
    "sequences_test = tokenizer.texts_to_sequences(testd_tweets)\n",
    "x_test_seq = pad_sequences(sequences_test, maxlen=length)\n",
    "ycnn01 = loaded_CNN_model.predict_classes(x=x_test_seq)\n",
    "ycnn02 = loaded_CNN_model.predict(x=x_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = open_file(\"train_pos_full.txt\")\n",
    "negative = open_file(\"train_neg_full.txt\")\n",
    "\n",
    "list_all_tweets = positive + negative\n",
    "y_full = [1] * len(positive) + [0] * len(negative)\n",
    "print(len(y))\n",
    "list_all_tweets, y_full = shuffle_tweets(list_all_tweets, y_full)\n",
    "print('shuffled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(max_features=100000, ngram_range=(1, 3))\n",
    "print('start fitting')\n",
    "tvec.fit(list_all_tweets)\n",
    "print('done fitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf = tvec.transform(list_all_tweets)\n",
    "x_test_tfidf = tvec.transform(testd_tweets)\n",
    "print('start LR')\n",
    "lr_with_tfidf = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "lr_with_tfidf.fit(x_train_tfidf, y_full)\n",
    "print('Predict LR')\n",
    "testy = lr_with_tfidf.predict(x_test_tfidf)\n",
    "save_csv('test_resulttfid.csv', testy)\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_lr = cross_validate(\n",
    "    LogisticRegression(solver='lbfgs', max_iter=500),\n",
    "    x_train_tfidf,\n",
    "    y_full,\n",
    "    return_train_score=False,\n",
    "    cv=5)\n",
    "#print(cv_results_lr['test_score'])\n",
    "print(np.mean(np.asarray(cv_results_lr['test_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
