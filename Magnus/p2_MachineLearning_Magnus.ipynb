{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification of tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Magnu\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, Doc2Vec, FastText\n",
    "import gensim.parsing.preprocessing as prep\n",
    "import csv\n",
    "import random\n",
    "#import pandas as pd\n",
    "#import spacy\n",
    "#import nltk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from numba import jit\n",
    "import numba as nb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets, svm\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "print('starting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all functions ready\n"
     ]
    }
   ],
   "source": [
    "def open_file(fileName):\n",
    "    with open(str(fileName), \"r\", encoding=\"utf8\") as sample:\n",
    "        s = sample.readlines() \n",
    "    return s\n",
    "\n",
    "def create_X(list_of_tweets, w2v, features):\n",
    "    \n",
    "    # this function needs some love!\n",
    "    \n",
    "    X = np.zeros((len(list_of_tweets),features)) \n",
    "    \n",
    "    for indeks, tweet in enumerate(list_of_tweets):\n",
    "        for word in tweet:\n",
    "            try:\n",
    "                X[indeks,:] = X[indeks,:] + model_tot.wv[str(word)]\n",
    "            except:\n",
    "                pass\n",
    "        N = len(tweet)\n",
    "        if N>0:\n",
    "            X[indeks] = X[indeks]/N\n",
    "    return X\n",
    "\n",
    "def createWordEmbedding(list_of_tweets, features, epoc, sg=0):\n",
    "    print(sg)\n",
    "    model = Word2Vec(list_of_tweets, size=features, window=5, min_count=1, workers=4, sg=sg)\n",
    "    print('word embedding created, start training:')\n",
    "    model.train(list_of_tweets, total_examples=len(list_of_tweets), epochs=epoc)\n",
    "    return model\n",
    "\n",
    "def createSentEmbedding(list_of_tweets, features, epoc):\n",
    "    model = Doc2Vec(list_of_tweets, size=features, min_count=1)\n",
    "    model.train(list_of_tweets, total_examples=len(list_of_tweets), epochs=epoc)\n",
    "    return model\n",
    "\n",
    "def save_csv(fileName, test_y):\n",
    "    ids = np.arange(len(test_y))  \n",
    "    with open(fileName, 'w') as csvfile:\n",
    "        tempwriter = csv.writer(csvfile)\n",
    "        tempwriter.writerow([\"Id\",\"Prediction\"])\n",
    "        count = 0\n",
    "        for row in test_y:\n",
    "            if row == 0:\n",
    "                row = -1\n",
    "            tempwriter.writerow([(ids[count])+1,str(row)])\n",
    "            count = count + 1\n",
    "            \n",
    "def train(method, x, y, x_test):\n",
    "    met = method.fit(x,y)\n",
    "    test_y = met.predict(x_test)\n",
    "    return test_y\n",
    "\n",
    "def shuffle_tweets(listTweets, y):\n",
    "    c = list(zip(listTweets, y))\n",
    "    random.shuffle(c)\n",
    "    listTweets, y = zip(*c)\n",
    "    return listTweets, y\n",
    "\n",
    "def processTrainingData(list_of_tweets):\n",
    "    # Does not seem to work:\n",
    "    #list_of_tweets = list(set(list_of_tweets)) # remove duplicate lines, should not be done for test-data (reorders the list)\n",
    "    #list_of_tweets = [prep.strip_multiple_whitespaces(line) for line in list_of_tweets]\n",
    "    \n",
    "    # May work, no clear improvement:\n",
    "    #list_of_tweets = [prep.strip_tags(line) for line in list_of_tweets]\n",
    "    #list_of_tweets = [prep.strip_numeric(line) for line in list_of_tweets]\n",
    "    #list_of_tweets = [prep.strip_non_alphanum(line) for line in list_of_tweets]\n",
    "    #list_of_tweets = [prep.split_alphanum(line) for line in list_of_tweets]\n",
    "    #list_of_tweets = [prep.strip_punctuation(line) for line in list_of_tweets]\n",
    "    #list_of_tweets = [prep.remove_stopwords(line) for line in list_of_tweets]\n",
    "    #list_of_tweets = [prep.stem_text(line) for line in list_of_tweets]\n",
    "    \n",
    "    # this seems to improve the result:\n",
    "    list_of_tweets = [prep.strip_short(line) for line in list_of_tweets]\n",
    "    \n",
    "    # Only one of these can be run simultaneously:\n",
    "    list_of_tweets = prep.preprocess_documents(list_of_tweets)\n",
    "    #list_of_tweets = [prep.preprocess_string(line) for line in list_of_tweets]\n",
    "    #list_of_tweets = [gensim.utils.simple_preprocess(line) for line in list_of_tweets] # simple preprocessing\n",
    "    return list_of_tweets\n",
    "\n",
    "def easyProcess(list_of_tweets):\n",
    "    \n",
    "    #list_of_tweets = [prep.strip_multiple_whitespaces(line) for line in list_of_tweets]\n",
    "    #list_of_tweets = [prep.strip_tags(line) for line in list_of_tweets]\n",
    "    #list_of_tweets = [prep.strip_numeric(line) for line in list_of_tweets]\n",
    "    #list_of_tweets = [prep.strip_non_alphanum(line) for line in list_of_tweets]\n",
    "    #list_of_tweets = [prep.split_alphanum(line) for line in list_of_tweets]\n",
    "    #list_of_tweets = [prep.strip_punctuation(line) for line in list_of_tweets]\n",
    "    #list_of_tweets = [prep.remove_stopwords(line) for line in list_of_tweets]\n",
    "    #list_of_tweets = [prep.stem_text(line) for line in list_of_tweets]\n",
    "    # this seems to improve the result:\n",
    "    list_of_tweets = [prep.strip_short(line) for line in list_of_tweets]\n",
    "\n",
    "    return list_of_tweets\n",
    "\n",
    "print('all functions ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "loaded and processed short training set\n"
     ]
    }
   ],
   "source": [
    "positive_tweets = open_file(\"train_pos.txt\")\n",
    "negative_tweets = open_file(\"train_neg.txt\")\n",
    "\n",
    "#list_all_tweets = positive_tweets+negative_tweets\n",
    "\n",
    "positive_tweets = processTrainingData(positive_tweets)\n",
    "\n",
    "negative_tweets = processTrainingData(negative_tweets)\n",
    "y = [1]*len(positive_tweets)+[0]*len(negative_tweets)\n",
    "print(len(y))\n",
    "\n",
    "#list_all_tweets, y_full = shuffle_tweets(list_all_tweets, y)\n",
    "\n",
    "\n",
    "all_tweets = positive_tweets+negative_tweets #list of tweets\n",
    "all_tweets, y = shuffle_tweets(all_tweets, y)\n",
    "print('loaded and processed short training set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list_all_tweets[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create word embedding based on full set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all loaded\n",
      "finished with processing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Creates word embeddings for full training set:\n",
    "\n",
    "positive_tweets_full = open_file(\"train_pos_full.txt\")\n",
    "negative_tweets_full = open_file(\"train_neg_full.txt\")\n",
    "print('all loaded')\n",
    "positive_tweets_full = processTrainingData(positive_tweets_full)\n",
    "negative_tweets_full = processTrainingData(negative_tweets_full)\n",
    "y_full = [1]*len(positive_tweets_full)+[0]*len(negative_tweets_full)\n",
    "print('finished with processing')\n",
    "all_tweets_full = positive_tweets_full+negative_tweets_full #list of tweets\n",
    "all_tweets_full, y_full = shuffle_tweets(all_tweets_full, y_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let the word embedding commence\n",
      "1\n",
      "word embedding created, start training:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "features = 200\n",
    "epoch = 10\n",
    "#print('let the word embedding commence')\n",
    "#model_tot_sg1 = createWordEmbedding(all_tweets_full, features, epoch, sg=1) #word embedding\n",
    "#model_tot_sg0 = createWordEmbedding(all_tweets_full, features, epoch) #word embedding\n",
    "#print('word embedding created')\n",
    "'''\n",
    "features = 350 # we should find optimal value for this:\n",
    "epoch = 10 # we should fin optimal value for this:\n",
    "model_tot = createWordEmbedding(all_tweets, features, epoch) #word embedding\n",
    "print('word embedding created')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_tot_sg1.save('model_tot_sg1_200.word2vec')\n",
    "#model_tot_sg0.save('model_tot_200.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tot = Word2Vec.load(\"model_tot_200.word2vec\")\n",
    "model_tot_sg1 = Word2Vec.load('model_tot_sg1_200.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_old = create_X(all_tweets,model_tot, features)\n",
    "X_old = preprocessing.scale(X_old)\n",
    "print('processed feature matrix for tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b = np.ones((X.shape[0], X.shape[1]+1)); b[:,:-1] = X\n",
    "X = np.ones((X_old.shape[0],X_old.shape[1]+1))\n",
    "X[:,1:] = X_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testd_tweets = open_file(\"test_data.txt\")\n",
    "testd = processTrainingData(testd_tweets)\n",
    "print('loaded test file')\n",
    "print(testd_tweets[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_old = create_X(testd, model_tot, features)\n",
    "X_test_old = preprocessing.scale(X_test_old)\n",
    "X_test = np.ones((X_test_old.shape[0],X_test_old.shape[1]+1))\n",
    "X_test[:,1:] = X_test_old\n",
    "print('start training')\n",
    "\n",
    "# Build logistic regression classifiers to identify the polarity of words\n",
    "#test_y_lr = train(LogisticRegression(solver='lbfgs', max_iter = 1000), X, y, X_test)\n",
    "\n",
    "# Build naive bayes classifiers to identify the polarity of words\n",
    "#test_y_nb = train(nb.GaussianNB(), X, y, X_test) # this one isn't working\n",
    "\n",
    "#print('created classification for submission')\n",
    "\n",
    "#save_csv('test_resultLR.csv', test_y_lr)\n",
    "#print('saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing some cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = svm.SVC(gamma='scale')\n",
    "\n",
    "#cv_results_clf = cross_validate(clf, X, y, return_train_score=False)\n",
    "#print(cv_results_clf['test_score'])\n",
    "\n",
    "#cv_results_rf = cross_validate(RandomForestClassifier(), X, y, return_train_score=False)\n",
    "#print(cv_results_rf['test_score'])\n",
    "\n",
    "cv_results_lr = cross_validate(LogisticRegression(solver = 'lbfgs'), X, y, return_train_score=False, cv = 5)\n",
    "#print(cv_results_lr['test_score'])\n",
    "print(np.mean(np.asarray(cv_results_lr['test_score'])))\n",
    "#cv_results_nb = cross_validate(nb.GaussianNB(), X, y, return_train_score=False)\n",
    "#print(cv_results_nb['test_score'])\n",
    "print('done with cross validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = open_file(\"train_pos.txt\")\n",
    "negative = open_file(\"train_neg.txt\")\n",
    "\n",
    "list_all_tweets = positive+negative\n",
    "y_full = [1]*len(positive)+[0]*len(negative)\n",
    "print(len(y))\n",
    "list_all_tweets, y_full = shuffle_tweets(list_all_tweets, y_full)\n",
    "list_all_tweets = easyProcess(list_all_tweets)\n",
    "print('shuffled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(list_all_tweets, y_full, test_size=.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train),len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_tot_k = KeyedVectors.load('model_tot.word2vec')\n",
    "model_tot_k2 = KeyedVectors.load('model_tot_sg1.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_tot_k.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "for w in model_tot_k.wv.vocab.keys():\n",
    "    embeddings_index[w] = np.append(model_tot_k.wv[w],model_tot_k2.wv[w])\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in x_train[:5]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "for x in x_train:\n",
    "    length.append(len(x.split()))\n",
    "max(length)\n",
    "length = max(length)+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_seq = pad_sequences(sequences, maxlen=length)\n",
    "print('Shape of data tensor:', x_train_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sequences_val = tokenizer.texts_to_sequences(x_validation)\n",
    "x_val_seq = pad_sequences(sequences_val, maxlen=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 100000\n",
    "embedding_matrix = np.zeros((num_words, features))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train), len(y_train), len(x_train_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hFeatures = features*0.5\n",
    "structure_test = Sequential()\n",
    "e = Embedding(100000, features, input_length=length)\n",
    "structure_test.add(e)\n",
    "structure_test.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "structure_test.add(GlobalMaxPooling1D())\n",
    "structure_test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_03 = Sequential()\n",
    "e = Embedding(100000, features, weights=[embedding_matrix], input_length=length, trainable=True)\n",
    "model_cnn_03.add(e)\n",
    "model_cnn_03.add(Conv1D(filters=features*0.5, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_03.add(GlobalMaxPooling1D())\n",
    "model_cnn_03.add(Dense(256, activation='relu'))\n",
    "model_cnn_03.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_03.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn_03.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=3, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.models import Model\n",
    "\n",
    "tweet_input = Input(shape=(length,), dtype='int32')\n",
    "\n",
    "tweet_encoder = Embedding(100000, features, weights=[embedding_matrix], input_length=length, trainable=True)(tweet_input)\n",
    "bigram_branch = Conv1D(filters=features*0.5, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=features*0.5, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=features*0.5, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = Dense(1)(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"CNN_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.fit(x_train_seq, y_train, batch_size=32, epochs=2,\n",
    "                     validation_data=(x_val_seq, y_validation), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded_CNN_model = load_model('CNN_best_weights.02-0.8333.hdf5')\n",
    "loaded_CNN_model.evaluate(x=x_val_seq, y=y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testd_tweets = easyProcess(testd_tweets)\n",
    "\n",
    "sequences_test = tokenizer.texts_to_sequences(testd_tweets)\n",
    "x_test_seq = pad_sequences(sequences_test, maxlen=length)\n",
    "ycnn01 = loaded_CNN_model.predict_classes(x=x_test_seq)\n",
    "ycnn02 = loaded_CNN_model.predict(x=x_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = open_file(\"train_pos_full.txt\")\n",
    "negative = open_file(\"train_neg_full.txt\")\n",
    "\n",
    "list_all_tweets = positive+negative\n",
    "y_full = [1]*len(positive)+[0]*len(negative)\n",
    "print(len(y))\n",
    "list_all_tweets, y_full = shuffle_tweets(list_all_tweets, y_full)\n",
    "print('shuffled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(max_features=100000,ngram_range=(1, 3))\n",
    "print('start fitting')\n",
    "tvec.fit(list_all_tweets)\n",
    "print('done fitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf = tvec.transform(list_all_tweets)\n",
    "x_test_tfidf = tvec.transform(testd_tweets)\n",
    "print('start LR')\n",
    "lr_with_tfidf = LogisticRegression(solver = 'lbfgs', max_iter = 1000)\n",
    "lr_with_tfidf.fit(x_train_tfidf,y_full)\n",
    "print('Predict LR')\n",
    "testy = lr_with_tfidf.predict(x_test_tfidf)\n",
    "save_csv('test_resulttfid.csv', testy)\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_lr = cross_validate(LogisticRegression(solver = 'lbfgs', max_iter = 500), x_train_tfidf, y_full, return_train_score=False, cv = 5)\n",
    "#print(cv_results_lr['test_score'])\n",
    "print(np.mean(np.asarray(cv_results_lr['test_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
