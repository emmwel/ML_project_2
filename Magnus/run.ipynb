{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification of tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Magnu\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from implementations import *\n",
    "from cnn import *\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, Doc2Vec, FastText, KeyedVectors\n",
    "import gensim.parsing.preprocessing as prep\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold, cross_validate\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Dense, concatenate, Activation, Dropout, Flatten\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create word embedding based on full set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 200 # number of features in word embedding\n",
    "epoch = 10 # number of epochs for word embedding\n",
    "\n",
    "#all_tweets_full, y_full = process_set('train_neg_full.txt', 'train_pos_full.txt') # corpus for word embedding\n",
    "#model_tot_200 = createWordEmbedding(all_tweets_full, features, epoch) # creates word embedding \n",
    "#model_tot_200.save('model_tot_200.word2vec') # saves word embedding\n",
    "#model_tot = Word2Vec.load('model_tot_200.word2vec') # loads word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation for word2vec + LR and tfidf + LR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/1\n",
      "180000/180000 [==============================] - 380s 2ms/step - loss: 0.4764 - acc: 0.7630 - val_loss: 0.4091 - val_acc: 0.8028\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.80280, saving model to cnn_result01_0.80.hdf5\n"
     ]
    }
   ],
   "source": [
    "# LR_with_w2v(model_tot, 'test_data.txt', 'train_neg.txt', 'train_pos.txt') # LR and word2vec\n",
    "# tfidf('test_data.txt', 'train_neg_full.txt', 'train_pos_full.txt') # TF-IDF with full set\n",
    "\n",
    "CNN('model_tot_200.word2vec', 'train_neg.txt', 'train_pos.txt', 512, 'test_data.txt', 1, 20000, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_tweets, y_full = easyProcess_set('train_neg_full.txt', 'train_pos_full.txt')\n",
    "SEED = 42\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(\n",
    "    list_all_tweets, y_full, test_size=.1, random_state=SEED)\n",
    "\n",
    "model_tot_k = KeyedVectors.load('model_tot_200.word2vec')\n",
    "\n",
    "embed_index = {}\n",
    "for w in model_tot_k.wv.vocab.keys():\n",
    "    embed_index[w] = model_tot_k.wv[w] # get index of words\n",
    "\n",
    "word_count = 100000\n",
    "tokenizer = Tokenizer(word_count)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "#Finds longest tweet:\n",
    "length = []\n",
    "for x in x_train:\n",
    "    length.append(len(x.split()))\n",
    "length = max(length)+2\n",
    "\n",
    "x_train_seq = pad_sequences(sequences, maxlen=length)\n",
    "\n",
    "sequences_val = tokenizer.texts_to_sequences(x_validation)\n",
    "x_val_seq = pad_sequences(sequences_val, maxlen=length)\n",
    "\n",
    "embed_matrix = np.zeros((word_count, features))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= word_count:\n",
    "        continue\n",
    "    embed_vector = embed_index.get(word)\n",
    "    if embed_vector is not None:\n",
    "        embed_matrix[i] = embed_vector\n",
    "        \n",
    "hFeatures = int(features * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_input = Input(shape=(length,), dtype='int32')\n",
    "\n",
    "tweet_encoder = Embedding(word_count, features, weights=[embed_matrix], input_length=length, trainable=True)(tweet_input)\n",
    "k2 = Conv1D(filters=hFeatures, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "k2 = GlobalMaxPooling1D()(k2)\n",
    "k4 = Conv1D(filters=hFeatures, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "k4 = GlobalMaxPooling1D()(k4)\n",
    "k6 = Conv1D(filters=hFeatures, kernel_size=6, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "k6 = GlobalMaxPooling1D()(k6)\n",
    "\n",
    "merged = concatenate([k2, k4, k6], axis=1)\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = Dense(1)(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filepath = \"CNN128_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.fit(\n",
    "    x_train_seq,\n",
    "    y_train,\n",
    "    batch_size=512,\n",
    "    epochs=4,\n",
    "    validation_data=(x_val_seq, y_validation),\n",
    "    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x=x_val_seq, y=y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_CNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_CNN_model.evaluate(x=x_val_seq, y=y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testd_tweets = open_file('test_data.txt')\n",
    "testd_tweets = easyProcess(testd_tweets)\n",
    "\n",
    "sequences_test = tokenizer.texts_to_sequences(testd_tweets)\n",
    "x_test_seq = pad_sequences(sequences_test, maxlen=length)\n",
    "ycnn02 = loaded_CNN_model.predict(x=x_test_seq)\n",
    "\n",
    "ycnn02_rounded = (np.around(ycnn02)).flatten()\n",
    "ycnn02_rounded[ycnn02_rounded == 0] = -1\n",
    "save_csv('test_resultCNN.csv', ycnn02_rounded)\n",
    "print(ycnn02)\n",
    "print(ycnn02_rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
